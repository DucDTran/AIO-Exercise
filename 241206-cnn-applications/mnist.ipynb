{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import safetensors\n",
    "# from torchsummary import summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "ROOT = './data'\n",
    "train_data = datasets.MNIST(\n",
    "    root=ROOT,\n",
    "    train=True,\n",
    "    download=True\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=ROOT,\n",
    "    train=False,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ratio for splitting training data into train and validation sets\n",
    "valid_ratio = 0.9\n",
    "\n",
    "# Calculate number of training examples based on ratio\n",
    "n_train_examples = int(len(train_data) * valid_ratio)\n",
    "# Calculate remaining examples for validation\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "train_data, valid_data = data.random_split(\n",
    "    train_data,\n",
    "    [n_train_examples, n_valid_examples]\n",
    ")\n",
    "\n",
    "# Calculate mean of training data and normalize to 0-1 range\n",
    "mean = train_data.dataset.data.float().mean() / 255\n",
    "# Calculate standard deviation of training data and normalize to 0-1 range\n",
    "std = train_data.dataset.data.float().std() / 255\n",
    "\n",
    "# Define transformations for training data - convert to tensor and normalize\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "# Define transformations for test data - same as training transforms\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[mean], std=[std])\n",
    "])\n",
    "\n",
    "# Apply training transformations to training dataset\n",
    "train_data.dataset.transform = train_transforms\n",
    "# Apply test transformations to validation dataset\n",
    "valid_data.dataset.transform = test_transforms\n",
    "\n",
    "# Set batch size for training\n",
    "batch_size = 256\n",
    "\n",
    "# Create training data loader with shuffling\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create validation data loader without shuffling\n",
    "val_dataloader = data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in train_dataloader:\n",
    "    print(inputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet CNN architecture for classification\n",
    "class LeNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        # Initialize parent class\n",
    "        super().__init__()\n",
    "        # First conv layer: 1 input channel (grayscale), 6 output channels, 5x5 kernel with same padding\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=6, kernel_size=5, padding='same'\n",
    "        )\n",
    "        # First pooling layer: 2x2 average pooling\n",
    "        self.avgpool1 = nn.AvgPool2d(kernel_size=2)\n",
    "        # Second conv layer: 6 input channels, 16 output channels, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6, out_channels=16, kernel_size=5\n",
    "        )\n",
    "        # Second pooling layer: 2x2 average pooling\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        # Flatten layer to convert 2D feature maps to 1D vector\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # First FC layer: 16*5*5=400 inputs (16 channels, 5x5 feature map), 120 outputs\n",
    "        self.fc_1 = nn.Linear(16*5*5, 120)\n",
    "        # Second FC layer: 120 inputs, 84 outputs\n",
    "        self.fc_2 = nn.Linear(120, 84)\n",
    "        # Output FC layer: 84 inputs, num_classes outputs\n",
    "        self.fc_3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs shape: (batch_size, 1, 28, 28)\n",
    "        \n",
    "        # Pass through first conv layer\n",
    "        # outputs shape: (batch_size, 6, 28, 28) - same padding preserves dimensions\n",
    "        outputs = self.conv1(inputs)\n",
    "        \n",
    "        # Apply first average pooling\n",
    "        # outputs shape: (batch_size, 6, 14, 14) - halved spatial dimensions\n",
    "        outputs = self.avgpool1(outputs)\n",
    "        \n",
    "        # Apply ReLU activation - shape remains (batch_size, 6, 14, 14)\n",
    "        outputs = F.relu(outputs)\n",
    "        \n",
    "        # Pass through second conv layer\n",
    "        # outputs shape: (batch_size, 16, 10, 10) - no padding reduces spatial dims by 4\n",
    "        outputs = self.conv2(outputs)\n",
    "        \n",
    "        # Apply second average pooling\n",
    "        # outputs shape: (batch_size, 16, 5, 5) - halved spatial dimensions\n",
    "        outputs = self.avgpool2(outputs)\n",
    "        \n",
    "        # Apply ReLU activation - shape remains (batch_size, 16, 5, 5)\n",
    "        outputs = F.relu(outputs)\n",
    "        \n",
    "        # Flatten 2D feature maps to 1D\n",
    "        # outputs shape: (batch_size, 16*5*5) = (batch_size, 400)\n",
    "        outputs = self.flatten(outputs)\n",
    "        \n",
    "        # Pass through first FC layer\n",
    "        # outputs shape: (batch_size, 120)\n",
    "        outputs = self.fc_1(outputs)\n",
    "        \n",
    "        # Pass through second FC layer\n",
    "        # outputs shape: (batch_size, 84)\n",
    "        outputs = self.fc_2(outputs)\n",
    "        \n",
    "        # Pass through output FC layer\n",
    "        # outputs shape: (batch_size, num_classes)\n",
    "        outputs = self.fc_3(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, optimizer, criterion, train_dataloader, device, epoch=0, log_interval=50):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "        total_count += labels.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(train_dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, valid_dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (inputs, labels) in enumerate(val_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            predictions = model(inputs)\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len(losses)\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/l9x2ykls3fd3qkgv7cpd1sgw0000gp/T/ipykernel_17025/84488631.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), 0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    50/  211 batches | accuracy    0.613\n",
      "| epoch   1 |   100/  211 batches | accuracy    0.877\n",
      "| epoch   1 |   150/  211 batches | accuracy    0.911\n",
      "| epoch   1 |   200/  211 batches | accuracy    0.934\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   1 | Time: 76.80s | Train Accuracy    0.942 | Train Loss    0.535 | Valid Accuracy    0.949 | Valid Loss    0.170 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    50/  211 batches | accuracy    0.953\n",
      "| epoch   2 |   100/  211 batches | accuracy    0.961\n",
      "| epoch   2 |   150/  211 batches | accuracy    0.966\n",
      "| epoch   2 |   200/  211 batches | accuracy    0.969\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   2 | Time: 73.00s | Train Accuracy    0.972 | Train Loss    0.124 | Valid Accuracy    0.970 | Valid Loss    0.098 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |    50/  211 batches | accuracy    0.974\n",
      "| epoch   3 |   100/  211 batches | accuracy    0.973\n",
      "| epoch   3 |   150/  211 batches | accuracy    0.972\n",
      "| epoch   3 |   200/  211 batches | accuracy    0.974\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   3 | Time: 78.70s | Train Accuracy    0.973 | Train Loss    0.087 | Valid Accuracy    0.979 | Valid Loss    0.070 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |    50/  211 batches | accuracy    0.977\n",
      "| epoch   4 |   100/  211 batches | accuracy    0.978\n",
      "| epoch   4 |   150/  211 batches | accuracy    0.979\n",
      "| epoch   4 |   200/  211 batches | accuracy    0.978\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   4 | Time: 77.05s | Train Accuracy    0.978 | Train Loss    0.072 | Valid Accuracy    0.983 | Valid Loss    0.057 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |    50/  211 batches | accuracy    0.982\n",
      "| epoch   5 |   100/  211 batches | accuracy    0.981\n",
      "| epoch   5 |   150/  211 batches | accuracy    0.981\n",
      "| epoch   5 |   200/  211 batches | accuracy    0.981\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   5 | Time: 86.06s | Train Accuracy    0.972 | Train Loss    0.062 | Valid Accuracy    0.982 | Valid Loss    0.055 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |    50/  211 batches | accuracy    0.984\n",
      "| epoch   6 |   100/  211 batches | accuracy    0.983\n",
      "| epoch   6 |   150/  211 batches | accuracy    0.982\n",
      "| epoch   6 |   200/  211 batches | accuracy    0.985\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   6 | Time: 80.52s | Train Accuracy    0.980 | Train Loss    0.054 | Valid Accuracy    0.985 | Valid Loss    0.046 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |    50/  211 batches | accuracy    0.984\n",
      "| epoch   7 |   100/  211 batches | accuracy    0.983\n",
      "| epoch   7 |   150/  211 batches | accuracy    0.986\n",
      "| epoch   7 |   200/  211 batches | accuracy    0.983\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   7 | Time: 79.06s | Train Accuracy    0.985 | Train Loss    0.051 | Valid Accuracy    0.989 | Valid Loss    0.039 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |    50/  211 batches | accuracy    0.987\n",
      "| epoch   8 |   100/  211 batches | accuracy    0.986\n",
      "| epoch   8 |   150/  211 batches | accuracy    0.985\n",
      "| epoch   8 |   200/  211 batches | accuracy    0.986\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   8 | Time: 78.62s | Train Accuracy    0.982 | Train Loss    0.046 | Valid Accuracy    0.985 | Valid Loss    0.046 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |    50/  211 batches | accuracy    0.988\n",
      "| epoch   9 |   100/  211 batches | accuracy    0.986\n",
      "| epoch   9 |   150/  211 batches | accuracy    0.987\n",
      "| epoch   9 |   200/  211 batches | accuracy    0.985\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   9 | Time: 79.33s | Train Accuracy    0.989 | Train Loss    0.042 | Valid Accuracy    0.987 | Valid Loss    0.041 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |    50/  211 batches | accuracy    0.989\n",
      "| epoch  10 |   100/  211 batches | accuracy    0.986\n",
      "| epoch  10 |   150/  211 batches | accuracy    0.985\n",
      "| epoch  10 |   200/  211 batches | accuracy    0.986\n",
      "-----------------------------------------------------------\n",
      "| End of epoch  10 | Time: 83.56s | Train Accuracy    0.989 | Train Loss    0.040 | Valid Accuracy    0.990 | Valid Loss    0.032 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_data.dataset.classes)\n",
    "\n",
    "lenet_model = LeNetClassifier(num_classes=num_classes)\n",
    "lenet_model.to(device=device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lenet_model.parameters())\n",
    "\n",
    "num_epochs = 10\n",
    "save_model = './model'\n",
    "\n",
    "train_accs, train_losses = [], []\n",
    "eval_accs, eval_losses = [], []\n",
    "best_loss_eval = 100\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    train_acc, train_loss = train(\n",
    "        model=lenet_model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_dataloader=train_dataloader,\n",
    "        device=device,\n",
    "        epoch=epoch\n",
    "    )\n",
    "\n",
    "    eval_acc, eval_loss = evaluate(\n",
    "        model=lenet_model,\n",
    "        criterion=criterion,\n",
    "        valid_dataloader=val_dataloader\n",
    "    )\n",
    "    eval_losses.append(eval_loss)\n",
    "\n",
    "    if eval_loss < best_loss_eval:\n",
    "        torch.save(lenet_model.state_dict(), save_model + '/lenet_model.pt')\n",
    "\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| End of epoch {:3d} | Time: {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} \"\n",
    "        \"| Valid Accuracy {:8.3f} | Valid Loss {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, train_acc, train_loss, eval_acc, eval_loss\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "\n",
    "    lenet_model.load_state_dict(safetensors.torch.load_model(save_model + \"/lenet_model.pt\"))\n",
    "    lenet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9896851851851852, 0.03167389450435884)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.transform = test_transforms\n",
    "test_dataloader = data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_acc, test_loss = evaluate(lenet_model, criterion, test_dataloader)\n",
    "test_acc, test_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
